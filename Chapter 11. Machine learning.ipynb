{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11. Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1 Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model is simply a specification of a mathematical or probabilistic relationship that exists between different variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, if you’re trying to raise money for your social networking site, you might build a business model (likely in a spreadsheet) that takes inputs like “number of users,” “ad revenue per user,” and “number of employees” and outputs your annual profit for the next several years. A cookbook recipe entails a model that relates inputs like “number of eaters” and “hungriness” to quantities of ingredients needed. And if you’ve ever watched poker on television, you know that each player’s “win probability” is estimated in real time based on a model that takes into account the cards that have been revealed so far and the distribution of cards in the deck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The business model is probably based on simple mathematical relationships: profit is revenue minus expenses, revenue is units sold times average price, and so on. The recipe model is probably based on trial and error—someone went in a kitchen and tried different combinations of ingredients until they found one they liked. And the poker model is based on probability theory, the rules of poker, and some reasonably innocuous assumptions about the random process by which cards are dealt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2 What is machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning to refer to creating and using models that are learned from data, also called predictive modeling or data mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3 Overfitting and underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "from typing import TypeVar, List, Tuple\n",
    "\n",
    "X = TypeVar('X') # generic type to represent a data point \n",
    "\n",
    "def split_data(data: List[X], prob: float) -> Tuple[List[X], List[X]]:\n",
    "    '''Split data into fractions [prob, 1 - prob]'''\n",
    "    data = data[:] # Make a shallow copy \n",
    "    random.shuffle(data) # Because shuffle modifies the list \n",
    "    cut = int(len(data) * prob) # Use prob to find a cutoff\n",
    "    return data[:cut], data[cut:] # Split the shuffled list \n",
    "\n",
    "data = [n for n in range(1000)]\n",
    "train, test = split_data(data, 0.75)\n",
    "\n",
    "assert len(train) == 750\n",
    "assert len(test) == 250\n",
    "assert sorted(train + test) == data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = TypeVar('Y') # generic type to represent output variables \n",
    "\n",
    "def train_test_split(xs: List[X],\n",
    "                     ys: List[Y],\n",
    "                     test_pct: float) -> Tuple[List[X], List[X], List[Y], List[Y]]:\n",
    "    # Generate the indices and split them \n",
    "    idxs = [i for i in range(len(xs))]\n",
    "    train_idxs, test_idxs = split_data(idxs, 1 - test_pct)\n",
    "    \n",
    "    return ([xs[i] for i in train_idxs], # x_train\n",
    "            [xs[i] for i in test_idxs], # x_test\n",
    "            [ys[i] for i in train_idxs], # y_train\n",
    "            [ys[i] for i in test_idxs] # y_test\n",
    "           )\n",
    "\n",
    "xs = [x for x in range(1000)]\n",
    "ys = [2 * x for x in xs]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(xs, ys, 0.25)\n",
    "            \n",
    "assert len(x_train) == len(y_train) == 750\n",
    "assert len(x_test) == len(y_test) == 250\n",
    "\n",
    "assert all(y == 2 * x for x, y in zip(x_train, y_train))\n",
    "assert all(y == 2 * x for x, y in zip(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SomeKindOfModel()\n",
    "x_train, x_test, y_train, y_test = train_test_split(xs, ys, 0.33)\n",
    "model.train(x_train, y_train)\n",
    "performance = model.test(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The common patterns in the test and training data that wouldn't generalize to a larger dataset: For example, imagine that your dataset consists of user activity, with one row per user per week. In such a case, most users will appear in both the training data and the test data, and certain models might learn to identify users rather than discover relationships involving attributes\n",
    "2. A bigger problem is if you use the test/train split not just to judge a model but also to choose from among many models. In that case, although each individual model may not be overfit, “choosing a model that performs best on the test set” is a meta-training that makes the test set function as a second training set. (Of course the model that performed best on the test set is going to perform well on the test set.) In such a situation, you should split the data into three parts: a training set for building models, a validation set for choosing among trained models, and a test set for judging the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.4 Correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix for binary label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(tp: int, fp: int, fn: int, tn: int) -> float:\n",
    "    correct = tp + tn\n",
    "    total = tp + fp + fn + tn\n",
    "    return correct / total\n",
    "\n",
    "assert accuracy(70, 4930, 13930, 981070) == 0.98114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(tp: int, fp: int, fn: int, tn: int) -> float:\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "assert precision(70, 4930, 13930, 981070) == 0.014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(tp: int, fp: int, fn: int, tn: int) -> float:\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "assert recall(70, 4930, 13930, 981070) == 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(tp: int, fp: int, fn: int, tn: int) -> float:\n",
    "    p = precision(tp, fp, fn, tn)\n",
    "    r = recall(tp, fp, fn, tn)\n",
    "    return 2 * p * r / (p + r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.5 The bias-variance tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High bias and low variance typically correspond to underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low bias and high variance typically correspond overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holding model complexity constant, the more data you have, the harder it is to overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, more data won't help with bias, if your model doesn't use enough features to capture regularities in the data, throwing more data at it won't help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.6 Feature extraction and selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features are whatever inputs we provide to our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The naive bayes classifier is suited to yes-or-no features \n",
    "2. Regression models require numeric features which could include dummy variables that are 0s and 1s\n",
    "3. Decision trees can deal with numeric or categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction and regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine experience and domain expertise to choose features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.7 For further exploration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
