{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9. Getting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 stdin and stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run your Python scripts at the command line, you can pipe data through them using sys.stdin and sys.stdout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/boyuan/anaconda3/envs/sds/lib/python3.8/site-packages/ipykernel_launcher.py',\n",
       " '-f',\n",
       " '/Users/boyuan/Library/Jupyter/runtime/kernel-695b37f1-9614-4d45-8a11-86f9590b3fed.json']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, re\n",
    "sys.argv\n",
    "# sys.argv is the list of command-line arguments\n",
    "# sys.argv[0] is the name of the program itself\n",
    "# sys.argv[1] will be the regex specified at the command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# egrep.py\n",
    "regex = sys.argv[1]\n",
    "for line in sys.stdin: # for every line passed into the script\n",
    "    if re.search(regex, line): \n",
    "        sys.stdout.write(line) # if it matches the regex, write it to stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# line_count.py \n",
    "import sys\n",
    "count = 0\n",
    "for line in sys.stdin:\n",
    "    count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat SomeFile.txt | python egrep.py \"[0-9]\" | python line_count.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The | is the pipe character, which means “use the output of the left command as the input of the right command.” You can build pretty elaborate data-processing pipelines this way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most_common_words.py\n",
    "\n",
    "import sys \n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# pass in number of words as first arguement \n",
    "\n",
    "try: \n",
    "    num_words = int(sys.argv[1])\n",
    "except:\n",
    "    print('usage: most_common_words.py num_words')\n",
    "    sys.exit(1) # nonzero exit code indicates error\n",
    "\n",
    "counter = Counter(word.lower()\n",
    "                  for line in sys.stdin\n",
    "                  for word in line.strip().split()\n",
    "                  if word)\n",
    "for word, count in counter.most_common(num_words):\n",
    "    sys.stdout.write(str(count))\n",
    "    sys.stdout.write(\"\\t\")\n",
    "    sys.stdout.write(word)\n",
    "    sys.stdout.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat the_bible.txt | python most_common_words.py 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Reading files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The basics of text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'r' means read-only, it's assumed if you leave it out\n",
    "file_for_reading = open('reading_file.txt', 'r')\n",
    "file_for_reading2 = open('reading_file.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'w' is write -- will destroy the file if it already exists!\n",
    "file_for_writing = open('writing_file.txt', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'a' is append -- for adding to the end of the file\n",
    "file_for_appending = open('appending_file.txt', 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't forget to close your files when you're done\n",
    "file_for_writing.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because it is easy to forget to close your files, you should always use them in a with block, at the end of which they will be closed automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename) as f:\n",
    "    data = function_that_gets_data_from(f)\n",
    "    \n",
    "# at this point f has already been closed, so don't try to use it\n",
    "process(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starts_with_hash = 0\n",
    "\n",
    "with open('input.txt') as f:\n",
    "    for line in f: # look at each line in the file\n",
    "        if re.match(\"^#\",line): # use a regex to see if it starts with '#'\n",
    "            starts_with_hash += 1 # if it does, add 1 to the count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domain(email_address: str) -> str:\n",
    "    \"\"\"Split on '@' and return the last piece\"\"\"\n",
    "    return email_address.lower().split(\"@\")[-1]\n",
    "\n",
    "# a couple of tests\n",
    "assert get_domain('joelgrus@gmail.com') == 'gmail.com'\n",
    "assert get_domain('joel@m.datasciencester.com') == 'm.datasciencester.com'\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "with open('email_addresses.txt', 'r') as f:\n",
    "    domain_counts = Counter(get_domain(line.strip())\n",
    "                            for line in f\n",
    "                            if \"@\" in line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delimited files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Never parse a comma-separated file yourself. You will screw up the edge cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "'''\n",
    "6/20/2014 AAPL 90.91\n",
    "6/20/2014 MSFT 41.68\n",
    "6/20/2014 FB 64.5\n",
    "6/19/2014 AAPL 91.86\n",
    "6/19/2014 MSFT 41.51\n",
    "6/19/2014 FB 64.34\n",
    "'''\n",
    "\n",
    "with open('tab_delimited_stock_prices.txt') as f:\n",
    "    tab_reader = csv.reader(f, delimiter = '\\t')\n",
    "    for row in tab_reader:\n",
    "        date = row[0]\n",
    "        symbol = row[1]\n",
    "        closing_price = float(row[2])\n",
    "        process(date, symbol, closing_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('colon_delimited_stock_prices.txt') as f:\n",
    "    colon_reader = csv.DictReader(f, delimiter=':')\n",
    "    for dict_row in colon_reader:\n",
    "        date = dict_row[\"date\"]\n",
    "        symbol = dict_row[\"symbol\"]\n",
    "        closing_price = float(dict_row[\"closing_price\"])\n",
    "        process(date, symbol, closing_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "todays_prices = {'AAPL': 90.91, 'MSFT': 41.68, 'FB': 64.5 }\n",
    "\n",
    "with open('comma_delimited_stock_prices.txt', 'w') as f:\n",
    "    csv_writer = csv.writer(f, delimiter=',')\n",
    "    for stock, price in todays_prices.items():\n",
    "        csv_writer.writerow([stock, price])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Scraping the web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML and the parsing thereof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pages on the web are written in HTML, in which text is (ideally) marked up into elements and their attributes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "    <head>\n",
    "        <title>A web page</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <p id=\"author\">Joel Grus</p>\n",
    "        <p id=\"subject\">Data Science</p>\n",
    "    </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a perfect world, where all web pages were marked up semantically for our benefit, we would be able to extract data using rules like “find the < p > element whose id is subject and return the text it contains.” In the actual world, HTML is not generally well formed, let alone annotated. This means we’ll need help making sense of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get data out of HTML, we will use the Beautiful Soup library, which builds a tree out of the various elements on a web page and provides a simple interface for accessing them. As I write this, the latest version is Beautiful Soup 4.6.0, which is what we’ll be using. We’ll also be using the Requests library, which is a much nicer way of making HTTP requests than anything that’s built into Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = (\"https://raw.githubusercontent.com/\"\n",
    "       \"joelgrus/data/master/getting-data.html\")\n",
    "html = requests.get(url).text\n",
    "soup = BeautifulSoup(html, 'html5lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<!DOCTYPE html>\n",
       "<html lang=\"en-US\"><head>\n",
       "    <title>Getting Data</title>\n",
       "    <meta charset=\"utf-8\"/>\n",
       "</head>\n",
       "<body>\n",
       "    <h1>Getting Data</h1>\n",
       "    <div class=\"explanation\">\n",
       "        This is an explanation.\n",
       "    </div>\n",
       "    <div class=\"comment\">\n",
       "        This is a comment.\n",
       "    </div>\n",
       "    <div class=\"content\">\n",
       "        <p id=\"p1\">This is the first paragraph.</p>\n",
       "        <p class=\"important\">This is the second paragraph.</p>\n",
       "    </div>\n",
       "    <div class=\"signature\">\n",
       "        <span id=\"name\">Joel</span>\n",
       "        <span id=\"twitter\">@joelgrus</span>\n",
       "        <span id=\"email\">joelgrus-at-gmail</span>\n",
       "    </div>\n",
       "\n",
       "\n",
       "</body></html>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p id=\"p1\">This is the first paragraph.</p>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_paragraph = soup.find('p')\n",
    "first_paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is the first paragraph.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_paragraph_text = soup.p.text\n",
    "first_paragraph_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'the', 'first', 'paragraph.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_paragraph_words = soup.p.text.split()\n",
    "first_paragraph_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'p1'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_paragraph_id = soup.p['id']\n",
    "first_paragraph_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'p1'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_paragraph_id2 = soup.p.get('id')\n",
    "first_paragraph_id2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"p1\">This is the first paragraph.</p>,\n",
       " <p class=\"important\">This is the second paragraph.</p>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_paragraphs = soup.find_all('p')\n",
    "all_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"p1\">This is the first paragraph.</p>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs_with_ids = [p for p in soup('p') if p.get('id')]\n",
    "paragraphs_with_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"important\">This is the second paragraph.</p>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_paragraphs = soup('p', {'class':'important'})\n",
    "important_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"important\">This is the second paragraph.</p>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_paragraphs2 = soup('p', 'important')\n",
    "important_paragraphs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"important\">This is the second paragraph.</p>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_paragraphs3 = [p for p in soup('p')\n",
    "                         if 'important' in p.get('class', [])]\n",
    "important_paragraphs3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<span id=\"name\">Joel</span>,\n",
       " <span id=\"twitter\">@joelgrus</span>,\n",
       " <span id=\"email\">joelgrus-at-gmail</span>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spans_inside_divs = [span\n",
    "                     for div in soup('div')\n",
    "                     for span in div('span')]\n",
    "spans_inside_divs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Keeping tabs on congress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = \"https://www.house.gov/representatives\"\n",
    "text = requests.get(url).text\n",
    "soup = BeautifulSoup(text, 'html5lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "966\n"
     ]
    }
   ],
   "source": [
    "all_urls = [a['href']\n",
    "            for a in soup('a')\n",
    "            if a.has_attr('href')]\n",
    "print(len(all_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "874\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Must start with http:// or https://\n",
    "# Must end with .house.gov or .house.gov/\n",
    "regex = r\"^https?://.*\\.house\\.gov/?$\"\n",
    "\n",
    "# Let's write some tests!\n",
    "assert re.match(regex, \"http://joel.house.gov\")\n",
    "assert re.match(regex, \"https://joel.house.gov\")\n",
    "assert re.match(regex, \"http://joel.house.gov/\")\n",
    "assert re.match(regex, \"https://joel.house.gov/\")\n",
    "assert not re.match(regex, \"joel.house.gov\")\n",
    "assert not re.match(regex, \"http://joel.house.com\")\n",
    "assert not re.match(regex, \"https://joel.house.gov/biography\")\n",
    "\n",
    "# And now apply\n",
    "good_urls = [url for url in all_urls if re.match(regex, url)]\n",
    "\n",
    "print(len(good_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "437\n"
     ]
    }
   ],
   "source": [
    "# Use set to get rid of the duplicates \n",
    "\n",
    "good_urls = list(set(good_urls))\n",
    "print(len(good_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'https://jayapal.house.gov/category/press-releases/', 'https://jayapal.house.gov/media/press-releases/'}\n"
     ]
    }
   ],
   "source": [
    "html = requests.get('https://jayapal.house.gov').text\n",
    "soup = BeautifulSoup(html, 'html5lib')\n",
    "\n",
    "# Use a set because the links might appear multiple times.\n",
    "links = {a['href'] for a in soup('a') if 'press releases' in a.text.lower()}\n",
    "\n",
    "print(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Set\n",
    "\n",
    "press_releases: Dict[str, Set[str]] = {}\n",
    "    \n",
    "for house_url in good_urls:\n",
    "    html = requests.get(house_url).text\n",
    "    soup = BeautifulSoup(html, 'html5lib')\n",
    "    pr_links = {a['href'] for a in soup('a') if 'press releases' in a.text.lower()}\n",
    "    \n",
    "    print(f\"{house_url}: {pr_links}\")\n",
    "    press_releases[house_url] = pr_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most sites will have a robots.txt file that indicates how frequently you may scrape the site (and which paths you’re not supposed to scrape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paragraph_mentions(text: str, keyword: str) -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if a <p> inside the text mentions {keyword}\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(text, 'html5lib')\n",
    "    paragraphs = [p.get_text() for p in soup('p')]\n",
    "    return any(keyword.lower() in paragraph.lower()\n",
    "               for paragraph in paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"<body><h1>Facebook</h1><p>Twitter</p>\"\"\"\n",
    "assert paragraph_mentions(text, \"twitter\") # is inside a <p>\n",
    "assert not paragraph_mentions(text, \"facebook\") # not inside a <p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://quigley.house.gov/\n",
      "https://long.house.gov/\n",
      "https://robinkelly.house.gov/\n",
      "https://foxx.house.gov/\n",
      "https://horsford.house.gov\n",
      "https://ruiz.house.gov\n",
      "https://cicilline.house.gov/\n",
      "https://riggleman.house.gov\n",
      "https://luria.house.gov\n",
      "https://chu.house.gov/\n",
      "https://castor.house.gov/\n",
      "https://crist.house.gov\n",
      "https://pascrell.house.gov/\n",
      "https://frankel.house.gov\n",
      "https://bass.house.gov/\n",
      "https://rubengallego.house.gov/\n",
      "https://buddycarter.house.gov/\n",
      "https://horn.house.gov/\n",
      "https://gosar.house.gov/\n"
     ]
    }
   ],
   "source": [
    "for house_url, pr_links in press_releases.items():\n",
    "    for pr_link in pr_links:\n",
    "        url = f\"{house_url}/{pr_link}\"\n",
    "        text = requests.get(url).text\n",
    "        \n",
    "        if paragraph_mentions(text, 'data'):\n",
    "            print(f\"{house_url}\")\n",
    "            break # done with this house_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4 Using APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many websites and web services provide application programming interfaces (APIs), which allow you to explicitly request data in a structured format. This saves you the trouble of having to scrape them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON and XML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because HTTP is a protocol for transferring text, the data you request through a web API needs to be serialized into a string format. Often this serialization uses JavaScript Object Notation (JSON). JavaScript objects look quite similar to Python dicts, which makes their string representations easy to interpret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{ \"title\" : \"Data Science Book\",\n",
    "  \"author\" : \"Joel Grus\",\n",
    "  \"publicationYear\" : 2019,\n",
    "  \"topics\" : [ \"data\", \"science\", \"data science\"] }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can parse JSON using Python’s json module. In particular, we will use its loads function, which deserializes a string representing a JSON object into a Python object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "serialized = '''\n",
    "{ \"title\" : \"Data Science Book\",\n",
    "  \"author\" : \"Joel Grus\",\n",
    "  \"publicationYear\" : 2019,\n",
    "  \"topics\" : [ \"data\", \"science\", \"data science\"] }\n",
    "'''\n",
    "\n",
    "# parse the JSON to create a python dict \n",
    "\n",
    "deserialized = json.loads(serialized)\n",
    "assert deserialized['publicationYear'] == 2019\n",
    "assert \"data science\" in deserialized['topics']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an unauthenticated API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 112873601,\n",
       " 'node_id': 'MDEwOlJlcG9zaXRvcnkxMTI4NzM2MDE=',\n",
       " 'name': 'advent2017',\n",
       " 'full_name': 'joelgrus/advent2017',\n",
       " 'private': False,\n",
       " 'owner': {'login': 'joelgrus',\n",
       "  'id': 1308313,\n",
       "  'node_id': 'MDQ6VXNlcjEzMDgzMTM=',\n",
       "  'avatar_url': 'https://avatars1.githubusercontent.com/u/1308313?v=4',\n",
       "  'gravatar_id': '',\n",
       "  'url': 'https://api.github.com/users/joelgrus',\n",
       "  'html_url': 'https://github.com/joelgrus',\n",
       "  'followers_url': 'https://api.github.com/users/joelgrus/followers',\n",
       "  'following_url': 'https://api.github.com/users/joelgrus/following{/other_user}',\n",
       "  'gists_url': 'https://api.github.com/users/joelgrus/gists{/gist_id}',\n",
       "  'starred_url': 'https://api.github.com/users/joelgrus/starred{/owner}{/repo}',\n",
       "  'subscriptions_url': 'https://api.github.com/users/joelgrus/subscriptions',\n",
       "  'organizations_url': 'https://api.github.com/users/joelgrus/orgs',\n",
       "  'repos_url': 'https://api.github.com/users/joelgrus/repos',\n",
       "  'events_url': 'https://api.github.com/users/joelgrus/events{/privacy}',\n",
       "  'received_events_url': 'https://api.github.com/users/joelgrus/received_events',\n",
       "  'type': 'User',\n",
       "  'site_admin': False},\n",
       " 'html_url': 'https://github.com/joelgrus/advent2017',\n",
       " 'description': 'advent of code 2017',\n",
       " 'fork': False,\n",
       " 'url': 'https://api.github.com/repos/joelgrus/advent2017',\n",
       " 'forks_url': 'https://api.github.com/repos/joelgrus/advent2017/forks',\n",
       " 'keys_url': 'https://api.github.com/repos/joelgrus/advent2017/keys{/key_id}',\n",
       " 'collaborators_url': 'https://api.github.com/repos/joelgrus/advent2017/collaborators{/collaborator}',\n",
       " 'teams_url': 'https://api.github.com/repos/joelgrus/advent2017/teams',\n",
       " 'hooks_url': 'https://api.github.com/repos/joelgrus/advent2017/hooks',\n",
       " 'issue_events_url': 'https://api.github.com/repos/joelgrus/advent2017/issues/events{/number}',\n",
       " 'events_url': 'https://api.github.com/repos/joelgrus/advent2017/events',\n",
       " 'assignees_url': 'https://api.github.com/repos/joelgrus/advent2017/assignees{/user}',\n",
       " 'branches_url': 'https://api.github.com/repos/joelgrus/advent2017/branches{/branch}',\n",
       " 'tags_url': 'https://api.github.com/repos/joelgrus/advent2017/tags',\n",
       " 'blobs_url': 'https://api.github.com/repos/joelgrus/advent2017/git/blobs{/sha}',\n",
       " 'git_tags_url': 'https://api.github.com/repos/joelgrus/advent2017/git/tags{/sha}',\n",
       " 'git_refs_url': 'https://api.github.com/repos/joelgrus/advent2017/git/refs{/sha}',\n",
       " 'trees_url': 'https://api.github.com/repos/joelgrus/advent2017/git/trees{/sha}',\n",
       " 'statuses_url': 'https://api.github.com/repos/joelgrus/advent2017/statuses/{sha}',\n",
       " 'languages_url': 'https://api.github.com/repos/joelgrus/advent2017/languages',\n",
       " 'stargazers_url': 'https://api.github.com/repos/joelgrus/advent2017/stargazers',\n",
       " 'contributors_url': 'https://api.github.com/repos/joelgrus/advent2017/contributors',\n",
       " 'subscribers_url': 'https://api.github.com/repos/joelgrus/advent2017/subscribers',\n",
       " 'subscription_url': 'https://api.github.com/repos/joelgrus/advent2017/subscription',\n",
       " 'commits_url': 'https://api.github.com/repos/joelgrus/advent2017/commits{/sha}',\n",
       " 'git_commits_url': 'https://api.github.com/repos/joelgrus/advent2017/git/commits{/sha}',\n",
       " 'comments_url': 'https://api.github.com/repos/joelgrus/advent2017/comments{/number}',\n",
       " 'issue_comment_url': 'https://api.github.com/repos/joelgrus/advent2017/issues/comments{/number}',\n",
       " 'contents_url': 'https://api.github.com/repos/joelgrus/advent2017/contents/{+path}',\n",
       " 'compare_url': 'https://api.github.com/repos/joelgrus/advent2017/compare/{base}...{head}',\n",
       " 'merges_url': 'https://api.github.com/repos/joelgrus/advent2017/merges',\n",
       " 'archive_url': 'https://api.github.com/repos/joelgrus/advent2017/{archive_format}{/ref}',\n",
       " 'downloads_url': 'https://api.github.com/repos/joelgrus/advent2017/downloads',\n",
       " 'issues_url': 'https://api.github.com/repos/joelgrus/advent2017/issues{/number}',\n",
       " 'pulls_url': 'https://api.github.com/repos/joelgrus/advent2017/pulls{/number}',\n",
       " 'milestones_url': 'https://api.github.com/repos/joelgrus/advent2017/milestones{/number}',\n",
       " 'notifications_url': 'https://api.github.com/repos/joelgrus/advent2017/notifications{?since,all,participating}',\n",
       " 'labels_url': 'https://api.github.com/repos/joelgrus/advent2017/labels{/name}',\n",
       " 'releases_url': 'https://api.github.com/repos/joelgrus/advent2017/releases{/id}',\n",
       " 'deployments_url': 'https://api.github.com/repos/joelgrus/advent2017/deployments',\n",
       " 'created_at': '2017-12-02T20:13:49Z',\n",
       " 'updated_at': '2019-10-29T09:45:54Z',\n",
       " 'pushed_at': '2017-12-26T15:02:27Z',\n",
       " 'git_url': 'git://github.com/joelgrus/advent2017.git',\n",
       " 'ssh_url': 'git@github.com:joelgrus/advent2017.git',\n",
       " 'clone_url': 'https://github.com/joelgrus/advent2017.git',\n",
       " 'svn_url': 'https://github.com/joelgrus/advent2017',\n",
       " 'homepage': None,\n",
       " 'size': 156,\n",
       " 'stargazers_count': 6,\n",
       " 'watchers_count': 6,\n",
       " 'language': 'Python',\n",
       " 'has_issues': True,\n",
       " 'has_projects': True,\n",
       " 'has_downloads': True,\n",
       " 'has_wiki': True,\n",
       " 'has_pages': False,\n",
       " 'forks_count': 2,\n",
       " 'mirror_url': None,\n",
       " 'archived': False,\n",
       " 'disabled': False,\n",
       " 'open_issues_count': 0,\n",
       " 'license': None,\n",
       " 'forks': 2,\n",
       " 'open_issues': 0,\n",
       " 'watchers': 6,\n",
       " 'default_branch': 'master'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests, json\n",
    "\n",
    "github_user = \"joelgrus\"\n",
    "endpoint = f\"https://api.github.com/users/{github_user}/repos\"\n",
    "\n",
    "repos = json.loads(requests.get(endpoint).text)\n",
    "repos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from dateutil.parser import parse\n",
    "\n",
    "dates = [parse(repo['created_at']) for repo in repos]\n",
    "month_counts = Counter(date.month for date in dates)\n",
    "weekday_counts = Counter(date.weekday() for date in dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({11: 5, 12: 4, 9: 4, 7: 4, 2: 3, 1: 3, 5: 3, 8: 2, 6: 1, 4: 1}) Counter({4: 7, 2: 7, 1: 5, 5: 4, 6: 4, 3: 2, 0: 1})\n"
     ]
    }
   ],
   "source": [
    "print(month_counts, weekday_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Python', 'JavaScript', 'Python', 'Python', 'Python']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_5_repositories = sorted(repos,\n",
    "                             key = lambda r: r['pushed_at'],\n",
    "                             reverse = True)[:5]\n",
    "last_5_languages = [repo['language']\n",
    "                    for repo in last_5_repositories]\n",
    "\n",
    "last_5_languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the benefits of using Python is that someone has already built a library for pretty much any API you’re interested in accessing. When they’re done well, these libraries can save you a lot of the trouble of figuring out the hairier details of API access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need data from a specific site, look for a “developers” or “API” section of the site for details, and try searching the web for “python < sitename > api” to find a library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.5 Example: Using the Twitter API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Caution:** Don’t share the keys, don’t publish them in your book, and don’t check them into your public GitHub repository. One simple solution is to store them in a credentials.json file that doesn’t get checked in, and to have your code use json.loads to retrieve them. Another solution is to store them in environment variables and use os.environ to retrieve them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a simple way, OAuth 2, that suffices when you just want to do simple searches. And there is a complex way, OAuth 1, that’s required when you want to perform actions (e.g., tweeting) or (in particular for us) connect to the Twitter stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twython import Twython\n",
    "from twython import TwythonStreamer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## 9.6 For further exploration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
